# -*- coding: utf-8 -*-
"""People Detection Using Yolov8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zNvHxlY1Uw4ylrWyGrHDItPI_irCeKFf

**Person Detection with Yolov8 - Assignment**

Name: Suprasanna V Gunaga

BE Information Science and Engineering

Email: suprasanna.gunaga@gmail.com

[LinkedIn](https://linkedin.com/in/suprasanna-v-gunaga-189831228)
    [GitHub](https://github.com/SuprasannaVG)
"""

!pip install ultralytics

"""**Making the predictions on Images using pretrained Yolov8s Model**

*Dataset*

100 images with people in diverse conditions.

70-training, 20-Validating, 10-Testing.
"""

import cv2
import matplotlib.pyplot as plt
from ultralytics import YOLO


model = YOLO('yolov8s.pt')


image_paths = [
    '/content/drive/MyDrive/Labellmg/Extra/OIP.jpeg',
    '/content/drive/MyDrive/Labellmg/Extra/00056dc4f587f43e.jpg',
    '/content/drive/MyDrive/Labellmg/Extra/OIP (3).jpeg'
]


for image_path in image_paths:

    img = cv2.imread(image_path)


    if img is None:
        print(f"Error: Could not load image at {image_path}")
    else:
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)


        results = model.predict(img_rgb)


        person_class_id =
        num_persons = sum(1 for cls in results[0].boxes.cls if cls == person_class_id)


        annotated_img = results[0].plot()


        plt.figure(figsize=(10, 10))
        plt.imshow(annotated_img)
        plt.axis('off')


        plt.figtext(0.5, 0.8, f'Number of Persons: {num_persons}', ha='center', va='bottom', fontsize=12, color='blue')
        plt.show()



"""**Computing the Evaluation Matrics**


*   Precision
*   Recall
*   F1 Score
*   mAP@50




"""

from ultralytics import YOLO
import matplotlib.pyplot as plt
from pathlib import Path

# Loading the pretrained YOLOv8 model
model = YOLO('yolov8s.pt')


custom_data_yaml = '/content/drive/MyDrive/Labellmg/data.yaml'

# Validating the model on the custom dataset
try:
    results = model.val(data=custom_data_yaml)
except Exception as e:
    print(f"Error during validation: {e}")
    results = None


if results:

    precision = results.results_dict.get('metrics/precision(B)', 0)
    recall = results.results_dict.get('metrics/recall(B)', 0)
    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # Avoid division by zero
    map50 = results.results_dict.get('metrics/mAP50(B)', 0)
    map50_95 = results.results_dict.get('metrics/mAP50-95(B)', 0)

    # Displaying metrics
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1_score:.4f}")
    print(f"mAP@50: {map50:.4f}")
    print(f"mAP@50-95: {map50_95:.4f}")

"""**Observation on computed metrics**
*  **Precision**: The model achieved a precision of 0.7645, indicating that approximately 76% of the predicted detections were correct.
*  **Recall**: The recall value of 0.6789 suggests that the model detected about 67% of the true objects present in the images.
*  **F1 Score**: The F1 score of 0.7191 shows a good balance between precision and recall.
*  **mAP@50**: A mAP@50 of 0.7133 indicates the model is performing reasonably well at a moderate level of IoU (0.5).
*  **mAP@50-95**: The mAP@50-95 of 0.3069 shows that the model struggles with tighter object overlaps and may need further improvement, particularly for detecting smaller or heavily occluded objects.



"""

